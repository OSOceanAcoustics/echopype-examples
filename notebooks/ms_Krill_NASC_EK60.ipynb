{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Krill NASC from Ship Echosounder Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook accompanying the manuscript:\n",
    "\n",
    "_Echopype: A Python library for interoperable and scalable processing of ocean sonar data for biological information_   \n",
    "_Authors: Wu-Jung Lee, Landung Setiawan, Caesar Tuguinay, Emilio Mayorga, and Valentina Staneva_\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "This notebook uses a 4-day subset of the EK60 echosounder data collected during the [2017 Joint U.S.-Canada Integrated Ecosystem and Pacific Hake Acoustic Trawl Survey (aka the \"Hake survey\")](https://www.fisheries.noaa.gov/west-coast/science-data/joint-us-canada-integrated-ecosystem-and-pacific-hake-acoustic-trawl-survey) to illustrate a processing pipeline that compute Nautical Areal Scattering Coefficient (NASC) attributed to krill. The workflow includes steps to perform data conversion, calibration, regridding, and frequency differencing, using functions from [`Echopype`](https://echopype.readthedocs.io) and core scientific Python software packages, particularly `NumPy`, `Xarray`, and `Dask`.\n",
    "\n",
    "### Outline\n",
    "\n",
    "1) Convert EK60 `.raw` files to `EchoData` objects\n",
    "2) Combine `EchoData` objects\n",
    "3) Calibrate raw backscatter measurement in the combined `EchoData` object to Sv\n",
    "4) Regrid calibrated Sv data to MVBS\n",
    "5) Generate and apply a frequency-differencing mask for krill\n",
    "6) Compute water column-integreated NASC for krill\n",
    "7) Plot water column-integrated krill NASC on the map\n",
    "\n",
    "### Running the notebook\n",
    "\n",
    "This notebook can be run with a conda environment created using the conda environment file https://github.com/OSOceanAcoustics/echopype-examples/blob/main/conda/environment-ep-main.yml. The notebook creates a directory, if not already present: `./exports/notebook_krill_outputs`. All Zarr files will be exported there.\n",
    "\n",
    "### Note\n",
    "We encourage importing `echopype` as `ep` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "import echopype as ep\n",
    "from echopype import colormap # needed for the EK500 colormap\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "# Ignore large graph dask UserWarnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish AWS S3 file system connection and generate list of target EK60 `.raw` files\n",
    "\n",
    "Access and inspect the publicly accessible NCEI WCSD S3 bucket on the AWS cloud as if it were a local file system. This will be done through the Python [fsspec](https://filesystem-spec.readthedocs.io) file system and bytes storage interface. We will use `fsspec.filesystem.glob` (`fs.glob`) to generate a list of all EK60 `.raw` data files in the bucket, then filter on file names for target dates of interest. \n",
    "\n",
    "The directory path on the [ncei-wcsd-archive S3 bucket](https://ncei-wcsd-archive.s3.amazonaws.com/index.html) is `s3://ncei-wcsd-archive/data/raw/Bell_M._Shimada/SH1707/EK60/`. All `.raw` files from the 2017 Hake survey cruise are found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 70 target raw files available\n"
     ]
    }
   ],
   "source": [
    "# Setup filesystem\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "bucket = \"ncei-wcsd-archive\"\n",
    "rawdirpath = \"data/raw/Bell_M._Shimada/SH1707/EK60\"\n",
    "\n",
    "# Select all RAW files from July 2017\n",
    "s3rawfiles = fs.glob(f\"{bucket}/{rawdirpath}/*.raw\")\n",
    "select_files = [\n",
    "    s3path\n",
    "    for s3path in s3rawfiles\n",
    "    if any(\n",
    "        f\"D2017{datestr}\" in s3path\n",
    "        for datestr in [\"0726\"]#, \"0727\", \"0728\", \"0729\"]\n",
    "    )\n",
    "]\n",
    "print(f\"There are {len(select_files)} target raw files available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These RAW files combined are about 8.1 GiB in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "If you are on Binder or are running this on a machine with less than 8 GiB of RAM, we recommend you to use only a subset of the `select_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to use a subset of the 324 files\n",
    "\n",
    "# select_files = select_files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the data below will be stored on disk, since it cannot fit in memory. Please modify the `base_path` below to point to disk storage location where you have **at least 50 GiB** of storage space available.\n",
    "\n",
    "Create directories to store Zarr files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"./exports/notebook_krill_outputs\")\n",
    "base_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "echodata_zarr_path = base_path / \"echodata_zarr\"\n",
    "echodata_zarr_path.mkdir(exist_ok=True)\n",
    "combined_zarr_path = base_path / \"combined_zarr\"\n",
    "combined_zarr_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Client Setup\n",
    "\n",
    "Echopype leverages Dask's lazy-load mechanisms to perform out-of-core and parallelized computation on large datasets. We use the Dask `Client` that is pointed to a `Scheduler` that schedules tasks and allocate memory for these computations.\n",
    "\n",
    "Due to the large volumes of data (if you did not subselect files above), if you have enough memory resources, set the `n_workers` to be such that each worker has at least 4 GiB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Client Dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Use maximum number of CPUs for Dask Client\n",
    "client = Client() # Set n_workers so that total_RAM / n_workers >= 4\n",
    "                  # or leave empty and let Dask decide\n",
    "print(\"Dask Client Dashboard:\", client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the Dask Client Dashboard link above to view the computations live.\n",
    "\n",
    "For more information on how to use, view, and interpret the Dashboard, go to this link: https://docs.dask.org/en/stable/dashboard.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data processing steps follow the typical Echopype workflow, which converts and standardizes data before performing more computation. Below we first convert and save the `.raw` files generated by EK60 to Zarr stores on disk, and lazy-load them back for downstream processing. This allows us to perform computations that may require resources that are larger than the local system memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert EK60 `.raw` Files to `EchoData` Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 4.87 s, total: 20.3 s\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Save EchoData objects locally\n",
    "def open_and_save(raw_file, sonar_model, use_swap, save_path, storage_options):\n",
    "    try:\n",
    "        ed = ep.open_raw(\n",
    "            raw_file=f's3://{raw_file}',\n",
    "            sonar_model=sonar_model,\n",
    "            use_swap=use_swap,\n",
    "            storage_options=storage_options,\n",
    "        )\n",
    "        ed.to_zarr(save_path, overwrite=True, compute=True)\n",
    "    except Exception as e:\n",
    "        print(\"Error with Exception: \", e)\n",
    "\n",
    "# Parse EK60 `.RAW` file and save to Zarr Store\n",
    "open_and_save_futures = []\n",
    "for raw_file_url in select_files:\n",
    "    open_and_save_future = client.submit(\n",
    "        open_and_save,\n",
    "        raw_file=raw_file_url,\n",
    "        sonar_model='ek60',\n",
    "        use_swap=True,\n",
    "        save_path=echodata_zarr_path,\n",
    "        storage_options={'anon': True}\n",
    "    )\n",
    "    open_and_save_futures.append(open_and_save_future)\n",
    "open_and_save_futures = client.gather(open_and_save_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Echodata Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood of the `ep.combine_echodata` function, the Zarr stores of each `EchoData` object are loaded and appended lazily, meaning that no data loading or appending actually happens until the combined object is saved to disk via `to_zarr`. Since the `EchoData` is appended sequentially based on the data time sequence and therefore not parallelized, this is the slowest stage of the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'echodata_zarr_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'echodata_zarr_path' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Open (lazy-load) Zarr stores containing EchoData Objects, and lazily combine them\n",
    "ed_future_list = []\n",
    "for converted_file in sorted(echodata_zarr_path.glob(\"*.zarr\")):\n",
    "    ed_future = client.submit(    \n",
    "        ep.open_converted,\n",
    "        converted_raw_path=converted_file,\n",
    "        chunks={}\n",
    "    )\n",
    "    ed_future_list.append(ed_future)\n",
    "ed_list = client.gather(ed_future_list)\n",
    "ed_combined = ep.combine_echodata(ed_list)\n",
    "\n",
    "# Save the combined EchoData object to a new Zarr store\n",
    "# The appending operation only happens when relevant data needs to be save to disk\n",
    "ed_combined.to_zarr(\n",
    "    combined_zarr_path / \"ed_combined.zarr\",\n",
    "    overwrite=True,\n",
    "    compute=True,\n",
    ")\n",
    "\n",
    "# Lazy-load the comined Zarr store\n",
    "ed_combined = ep.open_converted(\n",
    "    combined_zarr_path / \"ed_combined.zarr\",\n",
    "    chunks={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate Combined `EchoData` Object\n",
    "\n",
    "Compute Volume Backscattering Strength (Sv) from raw backscatter measurements stored in the converted `EchoData` object, and add depth and latitude/longitude to this Xarray Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.55 s, sys: 2.59 s, total: 10.1 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Chunk Echodata Object ping-wise\n",
    "ed_combined = ed_combined.chunk({\"ping_time\": 1000, \"range_sample\": -1})\n",
    "\n",
    "# Compute Sv\n",
    "ds_Sv = ep.calibrate.compute_Sv(ed_combined)\n",
    "\n",
    "# Add Depth\n",
    "ds_Sv = ep.consolidate.add_depth(ds_Sv, depth_offset=9.15)\n",
    "\n",
    "# Add Latitude and Longitude\n",
    "ds_Sv = ep.consolidate.add_location(ds_Sv, ed_combined)\n",
    "\n",
    "# Save to Zarr and offload computation to disk\n",
    "ds_Sv.to_zarr(\n",
    "    combined_zarr_path / \"ds_Sv.zarr\",\n",
    "    mode=\"w\",\n",
    "    compute=True,\n",
    ")\n",
    "\n",
    "# Lazy-load the Zarr store\n",
    "ds_Sv = xr.open_dataset(\n",
    "    combined_zarr_path / \"ds_Sv.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regrid Calibrated Data\n",
    "\n",
    "Compute Mean Volume Backscattering Strength (MVBS), which are binned averages of Sv (in the linear domain) across ping time and depth. This gets the data onto a common grid from the original ping times and depth sample size that may vary across ping and channel depending on the echosounder setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 3.59 s, total: 14.4 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Compute MVBS\n",
    "ds_MVBS = ep.commongrid.compute_MVBS(\n",
    "    ds_Sv,\n",
    "    range_var=\"depth\",\n",
    "    range_bin='5m', # in meters\n",
    "    ping_time_bin='20s', # in seconds\n",
    ")\n",
    "\n",
    "# Save to Zarr and offload computation to disk\n",
    "ds_MVBS.to_zarr(\n",
    "    combined_zarr_path / \"ds_MVBS.zarr\",\n",
    "    mode=\"w\",\n",
    "    compute=True,\n",
    ")\n",
    "\n",
    "# Lazy-load the Zarr store\n",
    "ds_MVBS = xr.open_dataset(\n",
    "    combined_zarr_path / \"ds_MVBS.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Apply a Frequency-Differencing Mask for Krill on the Echogram\n",
    "\n",
    "Frequency-differencing uses echo strength variation across frequency to classify the scatterers. The threshold of the difference of echo strength between observations from a pair of frequencies can be derived empirically or semi-empirically based on physics-based scattering models.\n",
    "\n",
    "Here we use the thresholds from Phillips et al. (2022) to identify krill:\n",
    "$$\n",
    "16.2 \\, \\textrm{dB} \\geq \\textrm{MVBS}_\\textrm{120kHz} - \\textrm{MVBS}_\\textrm{38kHz} \\geq 10.0 \\, \\textrm{dB}\n",
    "$$\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "Phillips EM, Chu D, Gauthier S, Parker-Stetter SL, Shelton AO, Thomas RE. (2022).\n",
    "Spatiotemporal variability of euphausiids in the California Current Ecosystem: insights from a recently developed time series. \n",
    "ICES Journal of Marine Science, 79(4): 1312â€“1326.\n",
    "https://doi.org/10.1093/icesjms/fsac055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the data is also thresholded to ensure that the frequency-differencing is applied on echogram pixels with reasonably high echo strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.93 ms, sys: 14 ms, total: 16.9 ms\n",
      "Wall time: 17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# MVBS threshold in dB\n",
    "th_MVBS = -70\n",
    "\n",
    "# Create copy of MVBS\n",
    "ds_non_empty_MVBS = ds_MVBS.copy()\n",
    "\n",
    "# Replace all Sv less -70 dB with NaN\n",
    "ds_non_empty_MVBS[\"Sv\"] = xr.where(ds_MVBS[\"Sv\"] < th_MVBS, np.nan, ds_MVBS[\"Sv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we use the Echopype functions to generate and apply masks to isolate krill on the echogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 220 ms, sys: 46.3 ms, total: 266 ms\n",
      "Wall time: 629 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Compute compute both frequency differencing inequalities as separate masks\n",
    "freq_diff_mask_1 = ep.mask.frequency_differencing(\n",
    "    ds_non_empty_MVBS,\n",
    "    freqABEq=\"120 kHz - 38 kHz < 16.2 dB\"\n",
    ")\n",
    "freq_diff_mask_2 = ep.mask.frequency_differencing(\n",
    "    ds_non_empty_MVBS,\n",
    "    freqABEq=\"120 kHz - 38 kHz > 10.0 dB\"\n",
    ")\n",
    "\n",
    "# Apply both frequency differencing masks\n",
    "ds_freq_diff_MVBS = ep.mask.apply_mask(\n",
    "    ds_non_empty_MVBS,\n",
    "    [freq_diff_mask_1, freq_diff_mask_2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Water Column-Integreated NASC for Krill\n",
    "\n",
    "NASC is typically computed along regular distance \"intervals\" and depth \"layers.\" Here, we use an interval of 0.5 nmi and a layer thickness of 10 m, and integrate the NASC values for the top 250 m of the water column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 625 ms, sys: 88.8 ms, total: 714 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Compute NASC\n",
    "ds_NASC = ep.commongrid.compute_NASC(\n",
    "    # Grab only the first 250m in the MVBS\n",
    "    ds_freq_diff_MVBS,\n",
    "    range_bin=\"10 m\",\n",
    "    dist_bin=\"0.5 nmi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 120 kHz channel\n",
    "ch_sel = [True if \"120\" in ch_id else False for ch_id in ds_NASC[\"channel\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate NASC at 120 kHz over the top 250 m of water column\n",
    "ds_NASC_int_120kHz = (\n",
    "    ds_NASC\n",
    "    .sel(depth=slice(0, 250)).isel(channel=ch_sel)\n",
    "    .squeeze().drop(\"channel\")\n",
    ") \n",
    "ds_NASC_int_120kHz[\"NASC\"] = (\n",
    "    ds_NASC_int_120kHz[\"NASC\"].sum(dim=\"depth\") * 10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attributes for automatic xarray plotting\n",
    "ds_NASC_int_120kHz[\"NASC\"].attrs = {\n",
    "    \"long_name\": \"Nautical Areal Scattering Coefficient (NASC)\",\n",
    "    \"units\": \"m2 nmi-2\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Water Column Krill dB NASC on map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting, let's plot the NASC values in the log domain so that we can see both the high and low values easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Log-transform NASC values\n",
    "ds_NASC_int_120kHz[\"NASC_log\"] = 10 * np.log10(ds_NASC_int_120kHz[\"NASC\"]).compute()\n",
    "ds_NASC_int_120kHz[\"NASC_log\"].attrs = {\n",
    "    \"long_name\": \"Log of NASC\",\n",
    "    \"units\": \"m2 nmi-2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the visualization below, the empty water columns (encoded as `NaN` in the NASC dataset) will be drawn with a translucent circle, and we use a dB scale to visualize the wide range of values in NASC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(12, 9), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Add coastlines and gridlines\n",
    "ax.coastlines()\n",
    "gl = ax.gridlines(\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True,\n",
    "    linewidth=1.5,\n",
    "    color='gray',\n",
    "    linestyle='--'\n",
    ")\n",
    "gl.top_labels = False\n",
    "gl.bottom_labels = True\n",
    "gl.left_labels = True\n",
    "gl.right_labels = False\n",
    "\n",
    "# Set lat lon extent\n",
    "ax.set_extent([\n",
    "    ds_NASC_int_120kHz[\"longitude\"].min().values - 0.3,\n",
    "    ds_NASC_int_120kHz[\"longitude\"].max().values + 0.3,\n",
    "    ds_NASC_int_120kHz[\"latitude\"].min().values - 0.3,\n",
    "    ds_NASC_int_120kHz[\"latitude\"].max().values + 0.3,\n",
    "], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add land and ocean features\n",
    "ax.add_feature(cfeature.LAND, color='green')\n",
    "ax.add_feature(cfeature.OCEAN, color='lightblue')\n",
    "\n",
    "# Plot dB NASC on map\n",
    "p = ds_NASC_int_120kHz.plot.scatter(\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    hue=\"NASC_log\",\n",
    "    ax=ax,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    linewidths=0.2,\n",
    "    cmap=\"Reds\"\n",
    ")\n",
    "\n",
    "# Set axes title\n",
    "p.axes.set_title(\"Krill NASC at 120kHz\", fontsize=16)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echopype: 0.8.5.dev82+gf9c1b27, xarray: 2024.6.0, dask: 2024.7.1\n",
      "\n",
      "2024-07-27 04:58:14.832113 +00:00\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import dask\n",
    "print(f\"echopype: {ep.__version__}, xarray: {xr.__version__}, dask: {dask.__version__}\")\n",
    "\n",
    "print(f\"\\n{datetime.datetime.utcnow()} +00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echopype",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
